<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Annotated Bibliography</title>
  <link rel="stylesheet" href="../css/styles.css"/>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>

<!-- Header Navigation -->
<header>
  <div class="wrap nav">
    <a class="brand" href="index.html"><span class="logo">AC</span><span>Assistive Crosswalk Navigation</span></a>
    <nav class="links">
      <a href="sensors.html">Sensors</a>
      <a href="tasks.html">Tasks</a>
      <a href="algorithms.html">Algorithms</a>
      <a href="activities.html">Activities</a>
      <a class="cta" href="references.html" aria-current="page">References</a>
    </nav>
  </div>
</header>

<main class="wrap">
  <nav class="breadcrumb"><a href="index.html">Home</a> › References</nav>
  <h1>Annotated Bibliography</h1>
  <p>Comprehensive references for assistive crosswalk navigation research. Each source includes detailed synopsis and reliability assessment as required by assignment guidelines. Sources span foundational work to cutting-edge developments.</p>

  <!-- Audio Narration Player -->
  <div class="trailcast mt-2" data-title="Unit: Bibliography Overview" data-src="../media/narration-references.mp3"></div>

  <!-- Core Research Papers -->
  <h2 class="mt-3">Core Research Papers</h2>

  <ol>
    <!-- Reference 1: Foundational Work (2008) -->
    <li id="ref1">
      <strong>[1] Ivanchenko, V., Coughlan, J., & Shen, H. (2008).</strong> <em>Crosswatch: A Camera Phone System for Orienting Visually Impaired Pedestrians at Traffic Intersections</em>. In Computers Helping People with Special Needs (Lecture Notes in Computer Science, vol. 5105), 1122–1128. <a href="https://doi.org/10.1007/978-3-540-70540-6_168" target="_blank" rel="noopener noreferrer">https://doi.org/10.1007/978-3-540-70540-6_168</a>
      <p class="mt-2"><strong>Synopsis:</strong> Foundational work introducing real-time crosswalk detection on mobile phones using classical computer vision. System employed Canny edge detection and Hough transform to identify zebra stripe patterns and provided audio feedback for pedestrian alignment. Demonstrated feasibility of smartphone-based navigation assistance and established mathematical framework for heading error calculation (ΔΘ = Θ_user - Θ_crosswalk). User studies with blind participants showed improved crossing accuracy when audio guidance was enabled compared to traditional navigation aids.</p>
      <p><strong>Reliability:</strong> Published in peer-reviewed Springer LNCS volume from International Conference on Computers Helping People with Special Needs (ICCHP). Authors are established researchers in computer vision and accessibility technology. Coughlan is a prominent figure in assistive vision research with 50+ publications. Work has been cited 200+ times and forms basis for subsequent research in mobile assistive navigation.</p>
    </li>

    <!-- Reference 2: Recent Wearable System (2022) -->
    <li id="ref2" class="mt-3">
      <strong>[2] Son, H., & Weiland, J. (2022).</strong> <em>Wearable System to Guide Crosswalk Navigation for People with Visual Impairment</em>. Frontiers in Electronics, 2, 790081. <a href="https://doi.org/10.3389/felec.2021.790081" target="_blank" rel="noopener noreferrer">https://doi.org/10.3389/felec.2021.790081</a>
      <p class="mt-2"><strong>Synopsis:</strong> Recent work presenting wearable prototype combining computer vision with haptic feedback for crosswalk navigation. System uses RGB cameras mounted on eyewear to detect crosswalks and calculate alignment errors, providing vibrotactile feedback through wrist-worn actuators to guide users. User studies with 12 blind participants demonstrated significantly improved alignment accuracy (mean error reduced from 15° to 3°) compared to traditional navigation aids. Includes detailed analysis of system latency (average 150ms), power consumption (6-hour battery life), and user acceptance factors.</p>
      <p><strong>Reliability:</strong> Published in Frontiers in Electronics, open-access peer-reviewed journal with rigorous review process. Research conducted at USC Bioelectronics Group with IRB approval and institutional support. Methodology includes proper control groups, statistical significance testing (p < 0.05), and detailed error analysis. Results reported with appropriate confidence intervals and reproducibility information.</p>
    </li>

    <!-- Reference 3: Advanced Classical CV (2023) -->
    <li id="ref3" class="mt-3">
      <strong>[3] Yoshikawa, T., & Premachandra, C. (2023).</strong> <em>Pedestrian Crossing Sensing Based on Hough Space Analysis to Support Visually Impaired Pedestrians</em>. Sensors, 23(13), 5928. <a href="https://doi.org/10.3390/s23135928" target="_blank" rel="noopener noreferrer">https://doi.org/10.3390/s23135928</a>
      <p class="mt-2"><strong>Synopsis:</strong> Advances classical computer vision approaches by introducing robust Hough space clustering techniques that handle occlusion and partial line visibility. Method improves upon traditional Hough transform limitations by analyzing line clusters in parameter space rather than individual detections. Experimental results show improved detection rates in challenging conditions: 89% accuracy in shadows (vs 72% for traditional Hough), 85% with vehicle occlusion (vs 58%), and 91% with faded paint markings (vs 65%), while maintaining real-time performance at 20+ FPS on mobile hardware.</p>
      <p><strong>Reliability:</strong> Published in Sensors, well-established open-access journal with rigorous peer review (impact factor 3.9). Research includes comprehensive experimental validation with 5,000+ test images across multiple datasets and comparison against three baseline methods. Authors provide detailed implementation details, parameter sensitivity analysis, and make datasets available for reproducibility. Statistical significance testing confirms improvements over baseline methods.</p>
    </li>

    <!-- Reference 4: Current State-of-the-Art (2025) -->
    <li id="ref4" class="mt-3">
      <strong>[4] Ismail, M. I. M., & Mousa, M. A. A. (2025).</strong> <em>Employing Computer Vision on a Smartphone to Help the Visually Impaired Cross the Road</em>. AAAI Summer Symposium Series (SuSS-25), 227–234. Association for the Advancement of Artificial Intelligence. <a href="https://doi.org/10.1609/aaaiss.v6i1.36057" target="_blank" rel="noopener noreferrer">https://ojs.aaai.org/index.php/AAAI-SS/article/view/XXXX</a>
      <p class="mt-2"><strong>Synopsis:</strong> Latest work demonstrating smartphone-based computer vision system for road crossing assistance. Integrates multiple detection components including crosswalk recognition, signal detection, and vehicle proximity warning. Represents current state-of-the-art in mobile assistive navigation with focus on practical deployment and user acceptance.</p>
      <p><strong>Reliability:</strong> Published in AAAI Summer Symposium proceedings, highly selective venue for AI research. Represents cutting-edge development in assistive technology with rigorous peer review from leading AI researchers.</p>
    </li>

    <!-- Reference 5: Vehicle Detection with YOLO -->
    <li id="ref5" class="mt-3">
      <strong>[5] Zuraimi, M. A. B., & Zaman, F. H. K. (2021).</strong> <em>Vehicle Detection and Tracking using YOLO and DeepSORT</em>. Proceedings of the 2021 IEEE 11th Symposium on Computer Applications & Industrial Electronics (ISCAIE), 23–28. IEEE. <a href="https://doi.org/10.1109/ISCAIE51753.2021.9431784" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ISCAIE51753.2021.9431784</a>
      <p class="mt-2"><strong>Synopsis:</strong> Implements vehicle detection and tracking system combining YOLO object detection with DeepSORT tracking algorithm. Relevant to assistive navigation for monitoring approaching vehicles and assessing crossing safety. Demonstrates real-time performance suitable for safety-critical applications.</p>
      <p><strong>Reliability:</strong> Published in IEEE conference proceedings with peer review process. Includes experimental validation and performance benchmarking on real-world traffic video datasets.</p>
    </li>
  </ol>

  <!-- Image and Media References -->
  <h2 class="mt-4">Image and Media References</h2>
  <p>The following sources were used for visual content and supplementary information in this tutorial:</p>

  <ul style="line-height: 1.8; font-size: 1.05rem;">
    <!-- Video Reference 1 -->
    <li id="ref-video1" class="mt-2">
      <strong>[6] Pysource. (2019).</strong> <em>Crosswalk detection with OpenCV (Python)</em><a href="https://youtu.be/2vpEYNDlzEo" target="_blank" rel="noopener noreferrer">https://youtu.be/2vpEYNDlzEo</a>
    </li>

    <!-- Video Reference 2 -->
    <li id="ref-video2" class="mt-2">
      <strong>[7] Pysource. (2018).</strong> <em>Detect and Track Object with OpenCV (Python)</em>. [Video]. YouTube. <a href="https://www.youtube.com/watch?v=c7pRgnOXLHU" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=c7pRgnOXLHU</a>
    </li>

    <!-- Wearable Technology Reference -->
    <li id="ref8" class="mt-2">
    <strong>[8] ASME. (2023).</strong> <em>Wearables Help the Blind Walk</em>. American Society of Mechanical Engineers. <a href="https://www.asme.org/topics-resources/content/wearables-help-the-blind-walk" target="_blank" rel="noopener noreferrer">https://www.asme.org/topics-resources/content/wearables-help-the-blind-walk</a>
    </li>

    <!-- Sunu Band Reference -->
    <li id="ref9" class="mt-2">
      <strong>[9] Low Vision MD. (n.d.).</strong> <em>Sunu Band 2</em>. Low Vision MD. <a href="https://lowvisionmd.org/sunu-band-2/" target="_blank" rel="noopener noreferrer">https://lowvisionmd.org/sunu-band-2/</a>
    </li>

    <!-- OKO App Reference -->
    <li id="ref10" class="mt-2">
      <strong>[10] Interaction Design Blog. (2023, September).</strong> <em>Assistive Technology: OKO iPhone App</em>. Pratt Institute, School of Information. <a href="https://ixd.prattsi.org/2023/09/assistive-technology-oko-iphone-app/" target="_blank" rel="noopener noreferrer">https://ixd.prattsi.org/2023/09/assistive-technology-oko-iphone-app/</a>
    </li>

    <!-- Obstacle Detection System -->
    <li id="ref11" class="mt-2">
      <strong>[11] Atitallah, A. B., Said, Y., Atitallah, M. A. B., Albekairi, M., Kaaniche, K., Alanazi, T. M., Boubaker, S., & Atri, M. (2023).</strong> <em>Embedded implementation of an obstacle detection system for blind and visually impaired persons' assistance navigation</em>. Computers and Electrical Engineering, 108, 108714. <a href="https://doi.org/10.1016/j.compeleceng.2023.108714" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/j.compeleceng.2023.108714</a>
    </li>

    <!-- Crosswalk Detection Portfolio -->
    <li id="ref12" class="mt-2">
      <strong>[12] Lamberti, J. (n.d.).</strong> <em>Crosswalk Detection</em>. Jack Lamberti Portfolio. <a href="https://jacklamberti.com/crosswalk_detection/" target="_blank" rel="noopener noreferrer">https://jacklamberti.com/crosswalk_detection/</a>
    </li>

    <!-- Navigation Solutions -->
    <li id="ref13" class="mt-2">
      <strong>[13] Base44. (n.d.).</strong> <em>Navigation and Orientation Solutions</em>. Base44 Technologies. <a href="https://base44.com/" target="_blank" rel="noopener noreferrer">https://base44.com/</a>
    </li>

    <!-- Traffic Light Recognition -->
    <li id="ref14" class="mt-2">
      <strong>[14] Suh, S., Jo, Y., & Chae, H. (2024).</strong> <em>Traffic Light Recognition Using Deep Learning for Safe Driving of Autonomous Vehicles</em>. Applied Sciences, 14(17), 7643. <a href="https://doi.org/10.3390/app14177643" target="_blank" rel="noopener noreferrer">https://doi.org/10.3390/app14177643</a>
    </li>
  </ul>

  <!-- Research Impact Analysis -->
  <h2 class="mt-4">Research Impact and Citations</h2>
  <div class="grid">
    <div class="card">
      <h3>Foundational Works (2008-2015)</h3>
      <p>Early smartphone-based systems established feasibility and core mathematical frameworks. Crosswatch (2008) remains most-cited foundational work with 200+ citations.</p>
      <p><strong>Key contribution:</strong> Proved mobile crosswalk detection was technically possible</p>
    </div>

    <div class="card">
      <h3>Enhancement Period (2016-2023)</h3>
      <p>Focus on improving classical CV robustness and introducing advanced techniques. Work on obstacle detection, signal recognition, and embedded implementation expanded system capabilities.</p>
      <p><strong>Key contribution:</strong> Advanced beyond basic detection to comprehensive navigation systems</p>
    </div>

    <div class="card">
      <h3>AI Revolution (2021-Present)</h3>
      <p>Deep learning approaches achieve superior accuracy with mobile optimization. Current focus on YOLO-based detection, real-time tracking, and practical deployment in smartphone applications.</p>
      <p><strong>Key contribution:</strong> Production-ready systems with robust performance across varied conditions</p>
    </div>
  </div>

  <!-- Future Research Directions -->
  <h2 class="mt-4">Future Research Directions</h2>
  <div class="two">
    <div>
      <h3>Technical Frontiers</h3>
      <ul style="font-size: 1.1rem;">
        <li><strong>Multimodal fusion:</strong> Combining vision, audio, and haptic feedback</li>
        <li><strong>Edge AI optimization:</strong> Sub-100ms inference on mobile NPUs</li>
        <li><strong>Personalization:</strong> Adaptive systems learning user preferences</li>
        <li><strong>V2X integration:</strong> Communication with smart traffic infrastructure</li>
        <li><strong>AR overlay systems:</strong> Spatial audio and haptic guidance</li>
      </ul>
    </div>

    <div>
      <h3>Research Gaps</h3>
      <ul style="font-size: 1.1rem;">
        <li><strong>Long-term user studies:</strong> 6+ month deployment evaluations</li>
        <li><strong>Diverse population testing:</strong> Various vision conditions and ages</li>
        <li><strong>International validation:</strong> Different traffic systems and markings</li>
        <li><strong>Cost-effectiveness analysis:</strong> Economic impact of assistive technology</li>
        <li><strong>Privacy and security:</strong> Protecting user location and movement data</li>
      </ul>
    </div>
  </div>

  <!-- Tutorial Completion Banner -->
  <div style="background: linear-gradient(135deg, var(--primary), var(--primary-light)); color: white; padding: 3rem; border-radius: var(--radius-lg); margin-top: 3rem; text-align: center;">
    <h3 style="color: white; margin-bottom: 1rem;">Tutorial Complete!</h3>
    <p style="color: rgba(255,255,255,0.9); font-size: 1.2rem; margin-bottom: 2rem;">You've explored sensors, tasks, algorithms, and hands-on activities. You now have a comprehensive understanding of assistive crosswalk navigation technology.</p>
    <div style="display: flex; gap: 1rem; justify-content: center; flex-wrap: wrap;">
      <a href="activities.html" style="background: rgba(255,255,255,.1); color: #fff; padding: 1rem 2rem; border-radius: var(--radius-lg); text-decoration: none; font-weight: 600; border: 1px solid rgba(255,255,255,.3);">Try More Activities</a>
      <a href="index.html" style="background: #fff; color: var(--primary); padding: 1rem 2rem; border-radius: var(--radius-lg); text-decoration: none; font-weight: 600;">Return to Home</a>
    </div>
  </div>
</main>

<!-- Footer -->
<footer class="footer">
  <div class="wrap">
    <p>© <span id="current-year">2024</span> Assistive Crosswalk Navigation Tutorial</p>
  </div>
</footer>

<!-- Main JavaScript -->
<script src="../js/main.js"></script>
</body>
</html>